<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Active3D</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-4 publication-title">ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D
                        Object Detection </h1>
                    <!-- <div class="is-size-5 publication-authors">
                      Paper authors
                      <span class="author-block">
                        <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                        <span class="author-block">
                          <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                          <span class="author-block">
                            <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                          </span>
                          </div> -->

                    <!-- <div class="is-size-5 publication-authors">
                      <span class="author-block">Institution Name<br>Conferance name and year</span>
                      <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    </div> -->

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- <span class="link-block">
                              <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                              class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span> -->

                            <!-- Supplementary PDF link -->
                            <!-- <span class="link-block">
                              <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                              class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                              </span>
                              <span>Supplementary</span>
                              </a>
                            </span> -->

                            <!-- Github link -->
                            <span class="link-block">
                    <a href="https://github.com/walzimmer/active3d-code" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>

                            <!-- Dataset link -->
                            <span class="link-block">
                    <a href="https://innovation-mobility.com/tumtraf-dataset" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Dataset</span>
                    </a>
                  </span>

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                              <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                                 class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <i class="ai ai-arxiv"></i>
                              </span>
                              <span>arXiv</span>
                              </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- title figure -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <img src="static/images/title_figure.jpg" alt="title_figure"/>
            </div>
        </div>
    </div>
</section>


<!-- Overview  -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Overview</h2>
                <div class="content">
                    <strong>ActiveAnno3D</strong> is the first active learning framework for multi-modal 3D object
                    detection. With this framework you can select data samples for labeling that are of maximum
                    informativeness for training.<br><br>
                    In summary:
                    <ul>
                        <li> We explore various continuous training methods and integrate the most efficient method
                            regarding computational demand and detection performance.
                        </li>
                        <li> We perform extensive experiments and ablation studies with <strong>BEVFusion</strong> and <strong>PV-RCNN</strong> on the
                            <strong>nuScenes</strong> and <strong>TUM Traffic Intersection</strong> datasets and show that we can achieve almost the same
                            performance when using only half of the data (<strong>79.65 mAP</strong> compared to <strong>83.50 mAP</strong>).
                        </li>
                        <li> We integrate our active learning framework into the <strong>proAnno</strong> labeling tool to enable
                            AI-assisted data selection and labeling and minimize the labeling costs.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End overview -->

<section class="section hero">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        The curation of large-scale datasets is still costly
                        and requires much time and resources. Data is often manually
                        labeled, and the challenge of creating high-quality datasets
                        remains. In this work, we fill the research gap using active
                        learning for multi-modal 3D object detection. We propose
                        ActiveAnno3D, an active learning framework to select data
                        samples for labeling that are of maximum informativeness for
                        training. We explore various continuous training methods and
                        integrate the most efficient method regarding computational
                        demand and detection performance. Furthermore, we perform
                        extensive experiments and ablation studies with BEVFusion
                        and PV-RCNN on the nuScenes and TUM Traffic Intersection
                        dataset. We show that we can achieve almost the same perfor
                        mance with PV-RCNN and the entropy-based query strategy
                        when using only half of the training data (77.25 mAP com
                        pared to 83.50 mAP) of the TUM Traffic Intersection dataset.
                        BEVFusion achieved an mAP of 64.31 when using half of the
                        training data and 75.0 mAP when using the complete nuScenes
                        dataset. We integrate our active learning framework into the
                        proAnno labeling tool to enable AI-assisted data selection and
                        labeling and minimize the labeling costs. Finally, we provide
                        code, weights, and visualization results on our website: https:
                        //active3d-framework.github.io/active3d-framework.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Architecture  -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <img src="static/images/architecture.jpg" alt="title_figure"/>
            </div>
        </div>
    </div>
</section>

<!-- Results  -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Evaluation</h2>
        <div class="item">
            <!-- include an html plot below -->
            <iframe src="static/html/interactive_plot_bevfusion.html" width="1000px" height="500px"
                    frameborder="0"></iframe>
            <h5 class="is-3 has-text-centered">BEVFusion on nuScenes dataset</h5>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="item">
            <!-- include an html plot below -->
            <iframe src="static/html/interactive_plot_pvrcnn.html" width="1000px" height="500px"
                    frameborder="0"></iframe>
        </div>
        <h5 class="is-3 has-text-centered">PV-RCNN on TUM Traffic Intersection dataset</h5>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="item">
            <img src="static/images/all_acquisition_functions_updated.jpg" alt="title_figure"/>
        </div>
        <h5 class="is-3 has-text-centered">The graph illustrates the mAP scores achieved by the PV-RCNN model on the TUM Traffic Intersection dataset relative to the expanding size of the training set in the active learning setting with different query strategies.</h5>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="item">
            <img src="static/images/qualitative_results" alt="title_figure"/>
        </div>
        <h5 class="is-3 has-text-centered">Qualitative results are illustrated by two pairs of images. The left pair is from the TUM Traffic Intersection dataset, and the right pair is from nuScenes. For each pair, the left image shows the predicted labels for each class, with each class represented by a different color. The right image of each pair shows the predictions made by learning on the complete dataset. Both results are quite similar, showing the efficiency of the active learning technique.</h5>
    </div>
</section>

<section class="hero">
    <div class="hero">
        <div class="container is-centered">
            <table class="table is-centered">
                <thead>
                <tr>
                    <th colspan="2">Labeled Pool</th>
                    <th colspan="2">LiDAR-Only (PV-RCNN)</th>
                    <th colspan="2">LiDAR+Camera (BEVFusion)</th>
                </tr>
                <tr>
                    <th>Round</th>
                    <th>%</th>
                    <th>Random</th>
                    <th>Entropy</th>
                    <th>Random</th>
                    <th>Entropy</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>1</td>
                    <td>10</td>
                    <td>51.03</td>
                    <td>54.32</td>
                    <td>30.95</td>
                    <td>31.06</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>15</td>
                    <td>61.98</td>
                    <td>62.24</td>
                    <td>34.19</td>
                    <td>36.39</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>20</td>
                    <td>69.84</td>
                    <td>68.23</td>
                    <td>38.00</td>
                    <td>40.41</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>25</td>
                    <td>74.82</td>
                    <td>72.40</td>
                    <td>42.36</td>
                    <td>42.17</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>30</td>
                    <td>77.25</td>
                    <td><u>76.56</u></td>
                    <td><strong>44.94</strong></td>
                    <td>45.57</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>35</td>
                    <td>75.40</td>
                    <td>75.00</td>
                    <td><u>44.74</u></td>
                    <td>46.76</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td>40</td>
                    <td><u>77.03</u></td>
                    <td>75.48</td>
                    <td>-</td>
                    <td><u>49.24</u></td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>50</td>
                    <td><strong>79.09</strong></td>
                    <td><strong>77.25</strong></td>
                    <td>-</td>
                    <td><strong>64.31</strong></td>
                </tr>
                <tr>
                    <td>SOA</td>
                    <td><strong>100</strong></td>
                    <td colspan="2"><strong>83.50</strong></td>
                    <td colspan="2"><strong>75.00</strong></td>
                </tr>
                </tbody>
            </table>
            <h2 class="subtitle has-text-centered">
                Evaluation of the PV-RCNN (LiDAR-only) and BEVFusion (camera+LiDAR) model using the random sampling
                baseline and entropy querying method on the TUM Traffic Intersection dataset and the nuScenes dataset.
                These Results are compaired to the respective 100% accuracies of the original work.
            </h2>
        </div>
    </div>
</section>
<!--BibTex citation -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>BibTex Code Here</code></pre>
  </div>
</section> -->
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content has-text-centered">
                    <p>
                        The <strong>Active3D</strong> framework is licensed under <a
                            href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA
                        4.0</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
